%! Date = 2/22/24
@article{Khorassani2023,
  author        = {Khorassani, Kawthar Shafie and Chen, Chen-Chun and Ramesh, Bharath and Shafi, Aamir and Subramoni, Hari and Panda, Dhabaleswar K.},
  title         = {High Performance MPI over the Slingshot Interconnect},
  journal       = {Journal of Computer Science and Technology},
  year          = {2023},
  month         = {Feb},
  day           = {01},
  volume        = {38},
  number        = {1},
  pages         = {128--145},
  abstract      = {The Slingshot interconnect designed by HPE/Cray is becoming more relevant in high-performance computing with its deployment on the upcoming exascale systems. In particular, it is the interconnect empowering the first exascale and highest-ranked supercomputer in the world, Frontier. It offers various features such as adaptive routing, congestion control, and isolated workloads. The deployment of newer interconnects sparks interest related to performance, scalability, and any potential bottlenecks as they are critical elements contributing to the scalability across nodes on these systems. In this paper, we delve into the challenges the Slingshot interconnect poses with current state-of-the-art MPI (message passing interface) libraries. In particular, we look at the scalability performance when using Slingshot across nodes. We present a comprehensive evaluation using various MPI and communication libraries including Cray MPICH, Open- MPI + UCX, RCCL, and MVAPICH2 on CPUs and GPUs on the Spock system, an early access cluster deployed with Slingshot-10, AMD MI100 GPUs and AMD Epyc Rome CPUs to emulate the Frontier system. We also evaluate preliminary CPU-based support of MPI libraries on the Slingshot-11 interconnect.},
  issn          = {1860-4749},
  doi           = {10.1007/s11390-023-2907-5},
  url           = {https://doi.org/10.1007/s11390-023-2907-5}
}
@article{DBLP:journals/corr/abs-2001-08361,
  author        = {Jared Kaplan and others},
  title         = {Scaling Laws for Neural Language Models},
  journal       = {CoRR},
  volume        = {abs/2001.08361},
  year          = {2020},
  url           = {https://arxiv.org/abs/2001.08361},
  eprinttype    = {arXiv},
  eprint        = {2001.08361},
  timestamp     = {Wed, 03 Jun 2020 10:55:13 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2005-14165,
  author        = {Tom B. Brown and others},
  title         = {Language Models are Few-Shot Learners},
  journal       = {CoRR},
  volume        = {abs/2005.14165},
  year          = {2020},
  url           = {https://arxiv.org/abs/2005.14165},
  eprinttype    = {arXiv},
  eprint        = {2005.14165},
  timestamp     = {Thu, 25 May 2023 10:38:31 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2006-16668,
  author        = {Dmitry Lepikhin and others},
  title         = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  journal       = {CoRR},
  volume        = {abs/2006.16668},
  year          = {2020},
  url           = {https://arxiv.org/abs/2006.16668},
  eprinttype    = {arXiv},
  eprint        = {2006.16668},
  timestamp     = {Thu, 02 Jul 2020 14:42:48 +0200},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2101-03961,
  author        = {William Fedus and Barret Zoph and Noam Shazeer},
  title         = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  journal       = {CoRR},
  volume        = {abs/2101.03961},
  year          = {2021},
  url           = {https://arxiv.org/abs/2101.03961},
  eprinttype    = {arXiv},
  eprint        = {2101.03961},
  timestamp     = {Thu, 21 Jan 2021 14:42:30 +0100},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2101-03961.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2105-04663,
  author        = {Yuanzhong Xu and HyoukJoong Lee and Dehao Chen and Blake A. Hechtman and Yanping Huang and Rahul Joshi and Maxim Krikun and Dmitry Lepikhin and Andy Ly and Marcello Maggioni and Ruoming Pang and Noam Shazeer and Shibo Wang and Tao Wang and Yonghui Wu and Zhifeng Chen},
  title         = {{GSPMD:} General and Scalable Parallelization for {ML} Computation Graphs},
  journal       = {CoRR},
  volume        = {abs/2105.04663},
  year          = {2021},
  url           = {https://arxiv.org/abs/2105.04663},
  eprinttype    = {arXiv},
  eprint        = {2105.04663},
  timestamp     = {Fri, 14 May 2021 12:13:30 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2105-04663.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2108-04690,
  author        = {Shangfeng Dai and Haobin Lin and Zhichen Zhao and Jianying Lin and Honghuan Wu and Zhe Wang and Sen Yang and Ji Liu},
  title         = {{POSO:} Personalized Cold Start Modules for Large-scale Recommender Systems},
  journal       = {CoRR},
  volume        = {abs/2108.04690},
  year          = {2021},
  url           = {https://arxiv.org/abs/2108.04690},
  eprinttype    = {arXiv},
  eprint        = {2108.04690},
  timestamp     = {Mon, 15 Aug 2022 13:20:49 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2108-04690.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2201-11990,
  author        = {Shaden Smith and others},
  title         = {Using DeepSpeed and Megatron to Train Megatron-Turing {NLG} 530B, {A} Large-Scale Generative Language Model},
  journal       = {CoRR},
  volume        = {abs/2201.11990},
  year          = {2022},
  url           = {https://arxiv.org/abs/2201.11990},
  eprinttype    = {arXiv},
  eprint        = {2201.11990},
  timestamp     = {Wed, 02 Feb 2022 15:00:01 +0100},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2201-11990.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/BengioLC13,
  author        = {Yoshua Bengio and Nicholas L{\'{e}}onard and Aaron C. Courville},
  title         = {Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation},
  journal       = {CoRR},
  volume        = {abs/1308.3432},
  year          = {2013},
  url           = {http://arxiv.org/abs/1308.3432},
  eprinttype    = {arXiv},
  eprint        = {1308.3432},
  timestamp     = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/BengioLC13.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{doi:10.1142/S0218001403002411,
  author        = {COLLOBERT, RONAN and BENGIO, YOSHUA and BENGIO, SAMY},
  title         = {SCALING LARGE LEARNING PROBLEMS WITH HARD PARALLEL MIXTURES},
  journal       = {International Journal of Pattern Recognition and Artificial Intelligence},
  volume        = {17},
  number        = {03},
  pages         = {349--365},
  year          = {2003},
  doi           = {10.1142/S0218001403002411},
  url           = {https://doi.org/10.1142/S0218001403002411}
}
@article{ShazeerMMDLHD17,
  author        = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc V. Le and Geoffrey E. Hinton and Jeff Dean},
  title         = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  journal       = {arXiv},
  volume        = {abs/1701.06538},
  year          = {2017},
  url           = {http://arxiv.org/abs/1701.06538},
  eprinttype    = {arXiv},
  eprint        = {1701.06538},
  timestamp     = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/ShazeerMMDLHD17.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@inbook{10.5555/3454287.3455008,
  author        = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K\"{o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  title         = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year          = {2019},
  publisher     = {Curran Associates Inc.},
  address       = {Red Hook, NY, USA},
  booktitle     = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  articleno     = {721},
  numpages      = {12}
}
@inproceedings{10.1145/3458817.3476209,
  author        = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
  title         = {Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM},
  year          = {2021},
  isbn          = {9781450384421},
  publisher     = {Association for Computing Machinery},
  address       = {New York, NY, USA},
  url           = {https://doi.org/10.1145/3458817.3476209},
  doi           = {10.1145/3458817.3476209},
  abstract      = {Large language models have led to state-of-the-art accuracies across several tasks. However, training these models efficiently is challenging because: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to scaling issues at thousands of GPUs. In this paper, we show how tensor, pipeline, and data parallelism can be composed to scale to thousands of GPUs. We propose a novel interleaved pipelining schedule that can improve throughput by 10+\% with memory footprint comparable to existing approaches. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs (per-GPU throughput of 52\% of theoretical peak).},
  booktitle     = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno     = {58},
  numpages      = {15},
  location      = {, St. Louis, Missouri,},
  series        = {SC '21}
}
@inproceedings{10.1145/3578244.3583736,
  author        = {Hao, Yueming and Jain, Nikhil and Van der Wijngaart, Rob and Saxena, Nirmal and Fan, Yuanbo and Liu, Xu},
  title         = {DrGPU: A Top-Down Profiler for GPU Applications},
  year          = {2023},
  isbn          = {9798400700682},
  publisher     = {Association for Computing Machinery},
  address       = {New York, NY, USA},
  url           = {https://doi.org/10.1145/3578244.3583736},
  doi           = {10.1145/3578244.3583736},
  booktitle     = {Proceedings of the 2023 ACM/SPEC International Conference on Performance Engineering},
  pages         = {43–53},
  numpages      = {11},
  keywords      = {GPU, measurement, profiler, performance optimization, CUDA},
  location      = {Coimbra, Portugal},
  series        = {ICPE '23}
}
@inproceedings{10.1145/3603269.3604869,
  author        = {Liu, Juncai and Wang, Jessie Hui and Jiang, Yimin},
  title         = {Janus: A Unified Distributed Training Framework for Sparse Mixture-of-Experts Models},
  year          = {2023},
  isbn          = {9798400702365},
  publisher     = {Association for Computing Machinery},
  address       = {New York, NY, USA},
  url           = {https://doi.org/10.1145/3603269.3604869},
  doi           = {10.1145/3603269.3604869},
  booktitle     = {Proceedings of the ACM SIGCOMM 2023 Conference},
  pages         = {486–498},
  numpages      = {13},
  keywords      = {distributed training, mixture of experts, deep learning},
  location      = {New York, NY, USA},
  series        = {ACM SIGCOMM '23}
}
@inproceedings{288705,
  author        = {Jiamin Li and Yimin Jiang and Yibo Zhu and Cong Wang and Hong Xu},
  title         = {Accelerating Distributed {MoE} Training and Inference with Lina},
  booktitle     = {2023 USENIX Annual Technical Conference (USENIX ATC 23)},
  year          = {2023},
  isbn          = {978-1-939133-35-9},
  address       = {Boston, MA},
  pages         = {945--959},
  url           = {https://www.usenix.org/conference/atc23/presentation/li-jiamin},
  publisher     = {USENIX Association},
  month         = jul
}
@inproceedings{NEURIPS2017_3f5ee243,
  author        = {Vaswani, Ashish and others},
  booktitle     = {Advances in Neural Information Processing Systems},
  title         = {Attention is All you Need},
  url           = {https://proceedings.neurips.cc/paper\_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume        = {30},
  year          = {2017}
}
@inproceedings{NEURIPS2021_48237d9f,
  author        = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr\'{e} and Keysers, Daniel and Houlsby, Neil},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages         = {8583--8595},
  title         = {Scaling Vision with Sparse Mixture of Experts},
  url           = {https://proceedings.neurips.cc/paper\_files/paper/2021/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf},
  volume        = {34},
  year          = {2021}
}
@inproceedings{NEURIPS2022_3e67e84a,
  author        = {Mustafa, Basil and others},
  booktitle     = {Advances in Neural Information Processing Systems},
  pages         = {9564--9576},
  publisher     = {Curran Associates, Inc.},
  title         = {Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts},
  url           = {https://proceedings.neurips.cc/paper\_files/paper/2022/file/3e67e84abf900bb2c7cbd5759bfce62d-Paper-Conference.pdf},
  volume        = {35},
  year          = {2022}
}
@inproceedings{pmlr-v162-du22c,
  title         = {{GL}a{M}: Efficient Scaling of Language Models with Mixture-of-Experts},
  author        = {Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten P and Zhou, Zongwei and Wang, Tao and Wang, Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and Meier-Hellstern, Kathleen and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
  booktitle     = {Proceedings of the 39th International Conference on Machine Learning},
  pages         = {5547--5569},
  year          = {2022},
  editor        = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume        = {162},
  series        = {Proceedings of Machine Learning Research},
  month         = {17--23 Jul},
  publisher     = {PMLR},
  pdf           = {https://proceedings.mlr.press/v162/du22c/du22c.pdf},
  url           = {https://proceedings.mlr.press/v162/du22c.html}
}
@inproceedings{pmlr-v162-rajbhandari22a,
  title         = {{D}eep{S}peed-{M}o{E}: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation {AI} Scale},
  author        = {Rajbhandari, Samyam and others},
  booktitle     = {Proceedings of the 39th International Conference on Machine Learning},
  pages         = {18332--18346},
  year          = {2022},
  month         = {17--23 Jul},
  publisher     = {PMLR},
  url           = {https://proceedings.mlr.press/v162/rajbhandari22a.html}
}
@inproceedings{Singh_2023,
  series        = {ICS '23},
  title         = {A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training},
  url           = {http://dx.doi.org/10.1145/3577193.3593704},
  doi           = {10.1145/3577193.3593704},
  booktitle     = {Proceedings of the 37th International Conference on Supercomputing},
  publisher     = {ACM},
  author        = {Singh, Siddharth and Ruwase, Olatunji and Awan, Ammar Ahmad and Rajbhandari, Samyam and He, Yuxiong and Bhatele, Abhinav},
  year          = {2023},
  month         = jun,
  collection    = {ICS '23}
}
@inproceedings{10.1145/2987550.2987554,
  author        = {Harlap, Aaron and Cui, Henggang and Dai, Wei and Wei, Jinliang and Ganger, Gregory R. and Gibbons, Phillip B. and Gibson, Garth A. and Xing, Eric P.},
  title         = {Addressing the straggler problem for iterative convergent parallel ML},
  year          = {2016},
  isbn          = {9781450345255},
  publisher     = {Association for Computing Machinery},
  address       = {New York, NY, USA},
  url           = {https://doi-org.proxy.library.cornell.edu/10.1145/2987550.2987554},
  doi           = {10.1145/2987550.2987554},
  abstract      = {FlexRR provides a scalable, efficient solution to the straggler problem for iterative machine learning (ML). The frequent (e.g., per iteration) barriers used in traditional BSP-based distributed ML implementations cause every transient slowdown of any worker thread to delay all others. FlexRR combines a more flexible synchronization model with dynamic peer-to-peer re-assignment of work among workers to address straggler threads. Experiments with real straggler behavior observed on Amazon EC2 and Microsoft Azure, as well as injected straggler behavior stress tests, confirm the significance of the problem and the effectiveness of FlexRR's solution. Using FlexRR, we consistently observe near-ideal run-times (relative to no performance jitter) across all real and injected straggler behaviors tested.},
  booktitle     = {Proceedings of the Seventh ACM Symposium on Cloud Computing},
  pages         = {98–111},
  numpages      = {14},
  location      = {Santa Clara, CA, USA},
  series        = {SoCC '16}
}
@misc{bengio2013deep,
  title         = {Deep Learning of Representations: Looking Forward},
  author        = {Yoshua Bengio},
  year          = {2013},
  eprint        = {1305.0445},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{gale2022megablocks,
  title         = {MegaBlocks: Efficient Sparse Training with Mixture-of-Experts},
  author        = {Trevor Gale and Deepak Narayanan and Cliff Young and Matei Zaharia},
  year          = {2022},
  eprint        = {2211.15841},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{Gemini_Team_2024,
  title         = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  url           = {https://storage.googleapis.com/deepmind-media/gemini/gemini\_v1\_5\_report.pdf},
  publisher     = {Google},
  author        = {Gemini},
  year          = {2024},
  month         = {Feb}
}
@misc{hwang2023tutel,
  title         = {Tutel: Adaptive Mixture-of-Experts at Scale},
  author        = {Changho Hwang and Wei Cui and Yifan Xiong and Ziyue Yang and Ze Liu and Han Hu and Zilong Wang and Rafael Salas and Jithin Jose and Prabhat Ram and Joe Chau and Peng Cheng and Fan Yang and Mao Yang and Yongqiang Xiong},
  year          = {2023},
  eprint        = {2206.03382},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DC}
}
@misc{kwon2023efficient,
  title         = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author        = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  year          = {2023},
  eprint        = {2309.06180},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@online{Open_AI_2024,
  url           = {https://openai.com/research/video-generation-models-as-world-simulators},
  title         = {Video generation models as world simulators},
  publisher     = {OpenAI},
  author        = {OpenAI},
  year          = {2024},
  month         = {Feb}
}
@online{PyT_2023,
  title         = {Torch.heaviside},
  url           = {https://pytorch.org/docs/stable/generated/torch.heaviside.html},
  publisher     = {PyTorch},
  author        = {PyTorch},
  year          = {2023}
}
@misc{puigcerver2023sparse,
  title         = {From Sparse to Soft Mixtures of Experts},
  author        = {Joan Puigcerver and Carlos Riquelme and Basil Mustafa and Neil Houlsby},
  year          = {2023},
  eprint        = {2308.00951},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{sanseviero2023moe,
  author        = {Omar Sanseviero and Lewis Tunstall and Philipp Schmid and Sourab Mangrulkar and Younes Belkada and Pedro Cuenca},
  title         = {Mixture of Experts Explained},
  year          = 2023,
  url           = {https://huggingface.co/blog/moe},
  publisher     = {Hugging Face Blog}
}
@misc{lewtunmixtral,
  author        = {Lewis Tunstall and Philipp Schmid and Omar Sanseviero and Pedro Cuenca and Olivier Dehaene and Leandro von Werra and Younes Belkada},
  title         = {Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face},
  year          = 2023,
  url           = {https://huggingface.co/blog/mixtral},
  publisher     = {Hugging Face Blog}
}
@misc{zoph2022stmoe,
  title         = {ST-MoE: Designing Stable and Transferable Sparse Expert Models},
  author        = {Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam Shazeer and William Fedus},
  year          = {2022},
  eprint        = {2202.08906},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{deepspeedcomm,
  author        = {DeepSpeed},
  title         = {Communication Logging},
  year          = {2024},
  lastaccessed  = {Mar 8, 2024},
  url           = {https://www.deepspeed.ai/tutorials/comms-logging/}
}
@online{mixtral8x7B,
  author        = {MistralAI},
  title         = {\textbf{Mixtral of experts}: A high quality Sparse Mixture-of-Experts.},
  year          = {2023},
  lastaccessed  = {Jan 7, 2024},
  url           = {https://mistral.ai/news/mixtral-of-experts/}
}
@online{azure,
  author        = {Azure},
  title         = {Updated NDv2-series},
  year          = {2022},
  lastaccessed  = {Feb 23, 2024},
  url           = {https://learn.microsoft.com/en-us/azure/virtual-machines/ndv2-series}
}
@online{perlm,
  author        = {NERSC},
  title         = {Perlmutter Architecture},
  year          = {2024},
  lastaccessed  = {Feb 23, 22024},
  url           = {https://docs.nersc.gov/systems/perlmutter/architecture/}
}
@online{nccl,
  author        = {NVIDIA},
  title         = {NCCL: Optimized primitives for inter-GPU communication.},
  year          = {2024},
  lastaccessed  = {Mar 8, 2024},
  url           = {https://github.com/NVIDIA/nccl}
}
@online{ncu,
  author        = {NVIDIA},
  title         = {Nsight Compute},
  year          = {2023},
  lastaccessed  = {Jan 8, 2024},
  url           = {https://docs.nvidia.com/nsight-compute/2023.3/}
}
@online{nsys,
  author        = {NVIDIA},
  title         = {Nsight Systems},
  year          = {2023},
  lastaccessed  = {Jan 8, 2024},
  url           = {https://docs.nvidia.com/nsight-systems/index.html}
}
@online{nvtx,
  author        = {NVIDIA},
  year          = {2022},
  title         = {NVTX},
  url           = {https://github.com/NVIDIA/NVTX/tree/release-v3/python}
}
