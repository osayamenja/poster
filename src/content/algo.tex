%! Date = 2/23/24

\section{Algorithm}\label{sec:algorithm}
\begin{table}[!h]
    \centering
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{ll}
            \toprule
            \textbf{Notation} & \textbf{Description} \\
            \midrule
            $H(x,\:y)$ & Heaviside Function $H(x)$, where $H(0) = y$\\
            $d$ & Data parallel degree, $d \in \mathbb{N}$\\
            $m$ & model parallel degree, $m \in \mathbb{N}$\\
            $k$ & Number of experts selected during token routing, $k \in \mathbb{N}$ see $top\_k$ in ~\cite{ShazeerMMDLHD17}\\
            $X$ & Set of all experts\\
            $N$ & Set of all nodes, $N \subset \mathbb{Z^+}$ \\
            $J_n$ & Set of all ranks in node $n$, $J_n \subset \mathbb{Z^+}$ \\
            $W$ & World of workers, note $|N| \cdot |J_n| = |W| = m \cdot d$ \\
            $\varepsilon$ & Optimism factor, $\varepsilon \in \{0, \ldots,\: |W| - 1\} \subset \mathbb{Z^+},$
            $\forall w \in W$ $\varepsilon_w = \varepsilon$\\
            $G_j^n$ & Worker with rank $j$ in node $n$ \\
            $X_j^n$ & Set of experts hosted on $G_j^n$, $X_j^n \subseteq X$ \\
            $\phi$ & Tensor mapping $G_j^n$ tokens $T$ to experts $X' \subseteq X$,
            $\phi \in \mathbb{R}^{|T| \times k}$  \\
            $S$ & Sharding specification tensor mapping $X$ to $W' \subseteq W$\\
            \bottomrule
        \end{tabular}
    }
    \caption{Notation}
    \label{tab:algonotation}
\end{table}
%% This declares a command \Comment
%% The argument will be surrounded by /* ... */
% \SetKwComment{Comment}{/* }{ */}
\SetKwInput{KwRequire}{Require}
\SetKw{kwAnd}{and}
\SetKw{kwTrue}{True}
\SetKw{kwFalse}{False}

\begin{algorithm}[!h]
    \DontPrintSemicolon
    \scriptsize
    \caption{Aristos; executed by each $G_j^n$}\label{alg:two}
    \KwData{$Q$: a blocking FIFO queue of tasks}
    \KwRequire{$X_j^n$, $S$, $\phi$, $\varepsilon$, $|W|$}
    \Begin{
        $\xi \gets \varepsilon$\;
        $G_\phi \gets \mathbf{GetWorkers}(\phi, \: S)$\;
        ${\mu} \gets |G_\phi| + H(|X_j^n|,\:0) \cdot (|W| - 1)$\;
        $flag \gets$ \kwFalse\;
        $W' \gets S.W'$\;
        \If{$G_j^n \in G_\phi$}{
            $\mu \gets \mu + 1$\;
            $\xi \gets \xi + 1$\;
        }
        $\mathbf{broadcast}(shouldProcess,\> \phi, \> G_\phi)$\;
        \While{$Q.timeout == $ \kwFalse \kwAnd $\mu > 0$}{
            \If{$flag == $ \kwFalse \kwAnd $\xi == 0$}{
                $Q.\mathbf{ActivateCountdown}()$\label{alg:countdown}\;
                $flag \gets $ \kwTrue\;
            }
            $t \gets Q.\mathbf{take()}$\label{alg:take}\;
            \uIf{$t.processed == $ \kwTrue}{
                $\mathbf{PostProcessing}(\phi,\> t)$\;
                $\xi \gets \xi - 1$\;
                $\mu \gets \mu - 1$\;
            }
            \uElseIf{$t.shouldProcess == $ \kwTrue}{
                $\theta = \mathbf{ExpertProcessing}(t, \> X_j^n)$\;
                $\mathbf{Send}(processed,\> \theta, \> t.workers)$\label{alg:send}\;
                $W' \gets W' - t.workers$\;
                $\mu \gets \mu - 1$\;
            }
        }
        \If{$\epsilon > 0$ \kwAnd $Q.timeout == $ \kwTrue}{
            $\mathbf{broadcast}(processed, \> W')$\label{alg:broadcast}\;
        }
    }
\end{algorithm}
In addition to pipelining, Aristos enables early-stopping via \emph{timeouts}, wherein a fast worker can discontinue the
collective communication and proceed to their next independent task,
albeit without updated logits for their tokens and dropping the tokens of other participating workers.
Aristos exposes a hyperparameter $\varepsilon$ to navigate this tradeoff between performance and token dropping.
We defer a complete overview and later optimizations of Aristos to the full version of the paper,
but briefly present the following.
\begin{theorem}\label{theo}
    Assume all processes $w \in W$ are correct with no failures, links are
    reliable and messages arrive in the order they are sent.
    If $\forall w \in W$, $\varepsilon_w = \varepsilon$, then Aristos satisfies liveness.
\end{theorem}
Define $\tau_i$ as the timeout time units for a process $p_i$.
Also, define a \emph{passive process} as one that has used their timeout and thus terminated the algorithm.
We start with an example and follow with a proof sketch.
Consider two processes $p_1$, $p_2$ where $p_1$ sets $\varepsilon_1=0$ and $p_2$ sets $\varepsilon_2=1$.
Consider the case where $p_1$, after executing line~\ref{alg:countdown},
times out and exits the loop, while $p_2$ blocks in line~\ref{alg:take}.
Note that $p_2$ will block indefinitely.
This would not be the case if $\varepsilon_1=\varepsilon_2=0$
(both eventually terminate) or $\varepsilon_1=\varepsilon_2=1$
The latter prevents blocking as well
because line ~\ref{alg:broadcast} forces the fastest of $p_1$ or $p_2$ to unblock the other process
which they did not receive from during their execution window.

\noindent\emph{Proof Sketch}.
Observe that if $\varepsilon_i =0$,
then any blocked process $p_i$ will eventually time out after $\tau_i$ and terminate the algorithm.
If $\varepsilon_i > 0$ then process $p_j$ who $p_i$ awaits, will eventually unblock $p_i$ after executing
either lines~\ref{alg:send} or ~\ref{alg:broadcast}.

Note that we foresee deriving $S$ by solving an optimization problem that automatically shards experts across
workers distributed per some topology denoted as a graph $G$.
Importantly, we aim to minimize communication latency $C$ and computation time $P$ for an end-to-end application,
training or inference, while satisfying memory and other yet to be determined constraints.

A simple sketch would be $ S = \min C + P$ subject to \[m_i \geq \sum_{e \in X} m_e \cdot x_{ei}, \> \forall i\in W\]
Note that $x_{ei}$ is a decision variable mapping expert $e$ to worker $i$, $m_i$ is the memory capacity of $i$ and
$m_e$ is the memory requirement for $e$.