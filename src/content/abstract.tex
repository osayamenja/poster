%! Date = 2/22/24

\begin{abstract}
    We propose \emph{Aristos}~\footnote{Translation of \textbf{optimal} in Greek}
    a communication-optimal, distributed algorithm that uses \emph{asynchronous communication}
    interleaved with computation for specifically tackling the communication overhead of
    Distributed Mixture-of-Experts (DMoE) transformer models.
    DMoE as implemented today entails frequent synchronous~\verb|All-to-All| communication operations
    that scale linearly~\emph{per step} with a model's number of layers.
    Thus, we first seek clarification on how~\verb|All-to-All| bottlenecks DMoE
    and whether the interconnect or communication algorithm is at fault.

    We investigate more than \textbf{21k}~\verb|All-to-All| CUDA kernels
    and empirically confirm that their runtimes exhibit a long-tail distribution
    with worst-case slow-down of
    \textbf{8.9}X and \textbf{100}X in multi-node and single-node settings, respectively.
    We argue that this phenomenon is a shortcoming of the \emph{global barrier}
    necessitated by the synchronous implementation of~\verb|All-to-All| in state of the art
    collective libraries such as NCCL\@.
    We use these empirical insights to motivate Aristos, which obviates the global barrier,
    instead favoring asynchronous communication yielding native support for pipelining.
    Aristos also exposes tunable hyperparameters that navigate the tradeoff of faster performance and
    reduced token dropping.
\end{abstract}