%! Author = jonathan
%! Date = 3/8/24

\section{Observations}\label{sec:observations}
Improving on the data report of related work~\cite{288705},
we use Nsight to profile over 21k CUDA~\verb|All-to-All| kernels spanning three training runs of two
GPT-3 MoE models: 350M and 1.3B, on Azure NDv2 and 32 GPUs across 8 NERSC Perlmutter nodes.
Note all experiments were performed on \emph{dedicated} hardware allocations without sharing.

Figure~\ref{fig:straggler} makes it clear that the interconnect is not the bottleneck,
but rather the synchronization delay due to the implicit barrier of synchronous ~\verb|All-to-All|.
We clarify that the multi-node results should be much \emph{worse} on commodity clusters that lack
the high-speed interconnect or RDMA stack of Perlmutter's Slingshot-11~\cite{Khorassani2023}.
